name: Digesto Inteligente – Pipeline Completo

on:
  schedule:
    - cron: "0 3 * * *"   # todos los días a las 00:00 Argentina
  workflow_dispatch:

jobs:
  digesto-master:
    runs-on: ubuntu-latest

    env:
      APP_KEY: ${{ secrets.APP_KEY }}
      APP_SECRET: ${{ secrets.APP_SECRET }}
      REFRESH_TOKEN: ${{ secrets.REFRESH_TOKEN }}

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Instalar dependencias
        run: |
          pip install pandas requests beautifulsoup4

      # ------------------------------
      # 1) Descargar Infoleg
      # ------------------------------
      - name: 1) Descargar Infoleg
        run: python scripts/descargar_infoleg.py

      # ------------------------------
      # 2) Procesar Infoleg
      # ------------------------------
      - name: 2) Procesar Infoleg
        run: python scripts/procesar_infoleg.py

      # ------------------------------
      # 3) Sincronizar fichas existentes
      # ------------------------------
      - name: 3) Sync fichas (actualiza columnas)
        run: python scripts/sync_fichas_dropbox.py

      # ------------------------------
      # 4) Descargar y parsear fichas que faltan
      #    Límite diario: 2000 normas para evitar rate limits
      # ------------------------------
      - name: 4) Procesar nuevas fichas
        run: |
          python - << 'EOF'
          import pandas as pd
          from scripts.scraper_fichas_infoleg import obtener_ficha

          df = pd.read_csv("data_procesada/digesto_normas.csv", dtype=str)

          faltan = df[df["ficha_parseada"] == "False"]["id_norma"].tolist()
          faltan = faltan[:10]  # límite diario

          print(f"Procesando {len(faltan)} fichas nuevas...")

          for i, norma in enumerate(faltan, 1):
              print(f"{i}/{len(faltan)}: {norma}")
              obtener_ficha(norma)
          EOF

      # ------------------------------
      # 5) Re-sincronizar columnas de fichas
      # ------------------------------
      - name: 5) Re-sync fichas
        run: python scripts/sync_fichas_dropbox.py

      # ------------------------------
      # 6) Subir digesto_relaciones.csv a dropbox
      # ------------------------------

      - name: Subir digesto_relaciones.csv a Dropbox
        run: |
          python - << 'EOF'
          import os, json, requests

          token_url = "https://api.dropbox.com/oauth2/token"
          upload_url = "https://content.dropboxapi.com/2/files/upload"

          APP_KEY = os.environ["APP_KEY"]
          APP_SECRET = os.environ["APP_SECRET"]
          REFRESH_TOKEN = os.environ["REFRESH_TOKEN"]

          # obtener access_token
          data = {
              "grant_type": "refresh_token",
              "refresh_token": REFRESH_TOKEN,
              "client_id": APP_KEY,
              "client_secret": APP_SECRET
          }
          r = requests.post(token_url, data=data)
          access = r.json()["access_token"]

          headers = {
              "Authorization": f"Bearer {access}",
              "Content-Type": "application/octet-stream",
              "Dropbox-API-Arg": json.dumps({
                  "path": "/data_procesada/digesto_relaciones.csv",
                  "mode": "overwrite",
                  "autorename": False
              })
          }

          with open("data_procesada/digesto_relaciones.csv", "rb") as f:
              requests.post(upload_url, headers=headers, data=f.read())

          EOF

        

      # ------------------------------
      # 7) Construir telaraña
      # ------------------------------
      - name: 7) Construir telaraña jurídica
        run: python scripts/construir_telaraña.py

      - name: FIN
        run: echo "Pipeline completo generado correctamente."
